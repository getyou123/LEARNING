### https://github.com/apache/flink-training.git

### 包结构说明
[entity](src%2Fmain%2Fjava%2Forg%2Fexample%2Fentity) 实体类类型
[ridecount](src%2Fmain%2Fjava%2Forg%2Fexample%2Fridecount) 进行count，看做是本项目的wordCount
[source](src%2Fmain%2Fjava%2Forg%2Fexample%2Fsource) 产生的数据源
[utils](src%2Fmain%2Fjava%2Forg%2Fexample%2Futils) 工具包

### 背景
- 出租车车程(taxi ride)事件结构[TaxiRide.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fentity%2FTaxiRide.java) 一个车程对应两个事件
- 出租车车费（taxi fare）事件结构[TaxiFare.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fentity%2FTaxiFare.java)

### 启动程序
[RideCountExample.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fridecount%2FRideCountExample.java)

### 练习1 过滤流
- DataStream.filter(FilterFunction) 核心是这个
- 可以写一个实现接口然后 `env.addSource(source).filter(new NYCFilter()).addSink(sink);`
``` 
 public static class NYCFilter implements FilterFunction<TaxiRide> {
        @Override
        public boolean filter(TaxiRide taxiRide) {
            return GeoUtils.isInNYC(taxiRide.startLon, taxiRide.startLat)
                    && GeoUtils.isInNYC(taxiRide.endLon, taxiRide.endLat);
        }
 }
```
- 可以继承 RichFilterFunction 这样的可以在open 和close中进行初始化和清理工作
``` 
public class MyFilter extends RichFilterFunction<Integer> {
    @Override
    public boolean filter(Integer value) throws Exception {
        return value % 2 == 0;
    }
}
```
-  Lambda 表达式
``` 
DataStream<Integer> stream = env.fromElements(1, 2, 3, 4);
DataStream<Integer> filtered = stream.filter(value -> value % 2 == 0);
```

### 如何构造一个产出指定类型的Source源
- [TaxiRideGenerator.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fsource%2FTaxiRideGenerator.java) 这个封装的不是很好因为更换源的时候不是很好更换

### 在编写job时候如何更好的切换测试和线上呢？
- 保留job中的source sink，然后new RideCleansingExercise时候来指定具体的sink 和 source
- 重写  RideCleansingExercise 中的 execute方法，代替env的
- [RideCleansingExercise.java](src%2Fmain%2Fjava%2Forg%2Fexample%2FexerciseOne%2FRideCleansingExercise.java) 这个封装的就很好，整个计算图中的source和sink都是构造实例时候传进来的

### 如何在测试过程中构造一个并行发送数据的source？
- 参考 [ParallelTestSource.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FParallelTestSource.java)，注意这里是子任务的概念和数组下标的对应方式
``` 
        TaxiRide toThePole = testRide(-73.9947F, 40.750626F, 0, 90);
        TaxiRide fromThePole = testRide(0, 90, -73.9947F, 40.750626F);
        TaxiRide atPennStation = testRide(-73.9947F, 40.750626F, -73.9947F, 40.750626F);
        TaxiRide atNorthPole = testRide(0, 90, 0, 90);

        // 这里构造了可以发送任何数据一个source，直接构造数据然后加入就可以获取到source
        ParallelTestSource<TaxiRide> source = new ParallelTestSource<>(toThePole, fromThePole, atPennStation, atNorthPole);
```
- ![](https://raw.githubusercontent.com/getyou123/git_pic_use/master/zz202304191645111.png)

### 练习二 有状态的增强(车程及车费)
- 背景主要为了实现一个开始的车程TaxiRide和车费TaxiFare进行匹配，构造产出一个车程车费对象RideAndFare
- 这里假设了 START 和 fare 事件完美配对
- [RidesAndFaresExercise.java](src%2Fmain%2Fjava%2Forg%2Fexample%2FexerciseTwo%2FRidesAndFaresExercise.java)
- [RidesAndFaresExerciseTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FRidesAndFaresExerciseTest.java)

### keyedStream
- 调用  keyBy(KeySelector) 实现，Key selector 函数接收单条记录作为输入，返回这条记录的 key。该 key 可以为任何类型，但是它的计算产生方式必须是具备确定性的
-  Key 是“虚拟”的。它们定义为基于实际数据的函数，用以操纵分组算子。 所以实际上不是把数据类型转为key-value形式

### 关于flink的state
- 算子状态
- Keyed DataStream：作用在指定的key上，
- 广播状态


--- 

### flink中的Keyed State 
keyed state 接口提供不同类型状态的访问接口，这些状态都作用于当前输入数据的 key 下。换句话说，这些状态仅可在 KeyedStream 上使用，在Java/Scala API上可以通过 stream.keyBy(...) 得到 KeyedStream，在Python API上可以通过 stream.key_by(...) 得到 KeyedStream。

接下来，我们会介绍不同类型的状态，然后介绍如何使用他们。所有支持的状态类型如下所示：

* ValueState<T>: 保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输入数据的 key，因此算子接收到的每个 key 都可能对应一个值）。 这个值可以通过 update(T) 进行更新，通过 T value() 进行检索。

* ListState<T>: 保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上进行检索。可以通过 add(T) 或者 addAll(List<T>) 进行添加元素，通过 Iterable<T> get() 获得整个列表。还可以通过 update(List<T>) 覆盖当前的列表。

* ReducingState<T>: 保存一个单值，表示添加到状态的所有值的聚合。接口与 ListState 类似，但使用 add(T) 增加元素，会使用提供的 ReduceFunction 进行聚合。

* AggregatingState<IN, OUT>: 保留一个单值，表示添加到状态的所有值的聚合。和 ReducingState 相反的是, 聚合类型可能与 添加到状态的元素的类型不同。 接口与 ListState 类似，但使用 add(IN) 添加的元素会用指定的 AggregateFunction 进行聚合。

* MapState<UK, UV>: 维护了一个映射列表。 你可以添加键值对到状态中，也可以获得反映当前所有映射的迭代器。使用 put(UK，UV) 或者 putAll(Map<UK，UV>) 添加映射。 使用 get(UK) 检索特定 key。 使用 entries()，keys() 和 values() 分别检索映射、键和值的可迭代视图。你还可以通过 isEmpty() 来判断是否包含任何键值对。


所有类型的状态还有一个clear() 方法，清除当前 key 下的状态数据，也就是当前输入元素的 key。

请牢记，这些状态对象仅用于与状态交互。状态本身不一定存储在内存中，还可能在磁盘或其他位置。 另外需要牢记的是从状态中获取的值取决于输入元素所代表的 key。 因此，在不同 key 上调用同一个接口，可能得到不同的值。

你必须创建一个 StateDescriptor，才能得到对应的状态句柄。 这保存了状态名称（正如我们稍后将看到的，你可以创建多个状态，并且它们必须具有唯一的名称以便可以引用它们）， 状态所持有值的类型，并且可能包含用户指定的函数，例如ReduceFunction。 根据不同的状态类型，可以创建ValueStateDescriptor，ListStateDescriptor， AggregatingStateDescriptor, ReducingStateDescriptor 或 MapStateDescriptor。

状态通过 RuntimeContext 进行访问，因此只能在 rich functions 中使用。请参阅这里获取相关信息， 但是我们很快也会看到一个例子。RichFunction 中 RuntimeContext 提供如下方法：

* ValueState<T> getState(ValueStateDescriptor<T>)
* ReducingState<T> getReducingState(ReducingStateDescriptor<T>)
* ListState<T> getListState(ListStateDescriptor<T>)
* AggregatingState<IN, OUT> getAggregatingState(AggregatingStateDescriptor<IN, ACC, OUT>)
* MapState<UK, UV> getMapState(MapStateDescriptor<UK, UV>)


示例程序： 
- [CountWindowAverage.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fhelloworld%2Fstate_learn%2FCountWindowAverage.java)
- [AvgByKeyedStateTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FAvgByKeyedStateTest.java)

### flink中的状态的有效期 TTL
任何类型的 keyed state 都可以有 有效期 (TTL)。如果配置了 TTL 且状态值已过期，则会尽最大可能清除对应的值，这会在后面详述。

所有状态类型都支持单元素的 TTL。 这意味着列表元素和映射元素将独立到期。

在使用状态 TTL 前，需要先构建一个StateTtlConfig 配置对象。 然后把配置传递到 state descriptor 中启用 TTL 功能
```
import org.apache.flink.api.common.state.StateTtlConfig;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.common.time.Time;

StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.seconds(1)) // 主要是时间是必选的
    .disableCleanupInBackground() // 可以开启 后台线程的清理
    //.cleanupFullSnapshot() 快照是开启清理
    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
    .build();
    
ValueStateDescriptor<String> stateDescriptor = new ValueStateDescriptor<>("text state", String.class);
stateDescriptor.enableTimeToLive(ttlConfig);
```

过期数据的清理：
- 全量快照时进行清理 .cleanupFullSnapshot()
- 增量数据清理  .cleanupIncrementally(10, true)
- 在 RocksDB 压缩时清 .cleanupInRocksdbCompactFilter(1000)


### 算子状态 (Operator State)
算子状态（或者非 keyed 状态）是绑定到一个并行算子实例的状态。Kafka Connector 是 Flink 中使用算子状态一个很具有启发性的例子。Kafka consumer 每个并行实例维护了 topic partitions 和偏移量的 map 作为它的算子状态。

当并行度改变的时候，算子状态支持将状态重新分发给各并行算子实例。处理重分发过程有多种不同的方案。

在典型的有状态 Flink 应用中你无需使用算子状态。它大都作为一种特殊类型的状态使用。用于实现 source/sink，以及无法对 state 进行分区而没有主键的这类场景中。


### 广播状态 (Broadcast State) 
广播状态是一种特殊的算子状态。引入它的目的在于支持一个流中的元素需要广播到所有下游任务的使用情形。在这些任务中广播状态用于保持所有子任务状态相同。 该状态接下来可在第二个处理记录的数据流中访问。可以设想包含了一系列用于处理其他流中元素规则的低吞吐量数据流，这个例子自然而然地运用了广播状态。 考虑到上述这类使用情形，广播状态和其他算子状态的不同之处在于：

它具有 map 格式，
它仅在一些特殊的算子中可用。这些算子的输入为一个广播数据流和非广播数据流，
这类算子可以拥有不同命名的多个广播状态 。


---
## 练习三 窗口分析 (每小时小费)
- 确定每小时赚取最多小费的司机，从 TaxiFare 流中按照窗口进行统计
- 所希望的结果是每小时产生一个 Tuple3<Long, Long, Float> 记录的数据流。 这个记录（Tuple3<Long, Long, Float>）应包含该小时结束时的时间戳（对应三元组的第一个元素）、 该小时内获得小费最多的司机的 driverId（对应三元组的第二个元素）以及他的实际小费总数（对应三元组的第三个元素））。
- 流转流的结构： 先开窗，然后按照司机id进行聚合，之后这个流在转为从每个司机流中获取最大的


### 误区排查
- 第一步都是按照小时进行开窗，计算窗内（不看前一个窗口）存在tips的driverId，然后数据输出的量级是：窗口内有driverId的计算窗口内的和
``` 
        // 构造的输入数据
        TaxiFare oneFor1In1 = testFare(1, t(0), 1.0F);
        TaxiFare fiveFor1In1 = testFare(1, t(15), 5.0F);
        ----
        TaxiFare tenFor1In2 = testFare(1, t(90), 10.0F);
        TaxiFare twentyFor2In2 = testFare(2, t(90), 20.0F);
        TaxiFare zeroFor3In2 = testFare(3, t(70), 0.0F);
        TaxiFare zeroFor4In2 = testFare(4, t(70), 0.0F);
        TaxiFare oneFor4In2 = testFare(4, t(80), 1.0F);
        TaxiFare tenFor5In2 = testFare(5, t(100), 10.0F);
        
       // 得到的数据输出是，这个是程序外面的时间线无关，因为是按照数据中事件时间算的
        (1577883600000,1,6.0), 
        ----
        (1577887200000,5,10.0), 
        (1577887200000,4,1.0), 
        (1577887200000,3,0.0), 
        (1577887200000,2,20.0), 
        (1577887200000,1,10.0) -- 这里没有driverId = 1 的数据因为上面窗口内没有这个数据
      
```
- 在获取到每个小时内每个司机的sum之后，接下来是需要注意的地方
``` 
  // find the driver with the highest sum of tips for each hour
        DataStream<Tuple3<Long, Long, Float>> hourlyMax =
                hourlyTips.windowAll(TumblingEventTimeWindows.of(Time.hours(1))).maxBy(2);

  -- 因为是开窗的操作，所以还是按照一个窗口内的输出一个 
   [(1577883600000,1,6.0), (1577887200000,2,20.0)]
   
   如果按照
   DataStream<Tuple3<Long, Long, Float>> hourlyMax = hourlyTips
    .keyBy(t -> t.f0)
    .maxBy(2);
    
    这个逻辑来写的话，那就会导致数据的输出是这样的
    (1577883600000,1,6.0), 
    (1577887200000,5,10.0), 
    (1577887200000,5,10.0), 
    (1577887200000,5,10.0), 
    (1577887200000,2,20.0), 
    (1577887200000,2,20.0)]
    可以看到是上游hourlyTips的每条记录都产出了一条记录，而不是两条记录
```


### 如何设置flink中的水印和时间戳呢？
- 一般来说，实际时间戳来自数据中的某个字段，使用TimestampAssigner去设置
- 同时时间戳的分配与 watermark 的生成是齐头并进的，水印用来告知flink程序的时间进度，使用 WatermarkGenerator 来设置
- `WatermarkStrategy` 工具类整合了 `TimestampAssigner` 和 `WatermarkGenerator` ，在source时候直接设置获取水印和时间戳
``` 
WatermarkStrategy
        .<Tuple2<Long, String>>forBoundedOutOfOrderness(Duration.ofSeconds(20)) // 最大混乱程度的
        .withTimestampAssigner((event, timestamp) -> event.f0);
```
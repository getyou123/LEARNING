### https://github.com/apache/flink-training.git

### 包结构说明

[entity](src%2Fmain%2Fjava%2Forg%2Fexample%2Fentity) 实体类类型
[ridecount](src%2Fmain%2Fjava%2Forg%2Fexample%2Fridecount) 进行count，看做是本项目的wordCount
[source](src%2Fmain%2Fjava%2Forg%2Fexample%2Fsource) 产生的数据源
[utils](src%2Fmain%2Fjava%2Forg%2Fexample%2Futils) 工具包

### 背景

- 出租车车程(taxi ride)事件结构[TaxiRide.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fentity%2FTaxiRide.java) 一个车程对应两个事件
- 出租车车费（taxi fare）事件结构[TaxiFare.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fentity%2FTaxiFare.java)

### 启动程序

[RideCountExample.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fridecount%2FRideCountExample.java)

### 练习1 过滤流

- DataStream.filter(FilterFunction) 核心是这个
- 可以写一个实现接口然后 `env.addSource(source).filter(new NYCFilter()).addSink(sink);`

``` 
 public static class NYCFilter implements FilterFunction<TaxiRide> {
        @Override
        public boolean filter(TaxiRide taxiRide) {
            return GeoUtils.isInNYC(taxiRide.startLon, taxiRide.startLat)
                    && GeoUtils.isInNYC(taxiRide.endLon, taxiRide.endLat);
        }
 }
```

- 可以继承 RichFilterFunction 这样的可以在open 和close中进行初始化和清理工作

``` 
public class MyFilter extends RichFilterFunction<Integer> {
    @Override
    public boolean filter(Integer value) throws Exception {
        return value % 2 == 0;
    }
}
```

- Lambda 表达式

``` 
DataStream<Integer> stream = env.fromElements(1, 2, 3, 4);
DataStream<Integer> filtered = stream.filter(value -> value % 2 == 0);
```

--- 

### 如何构造一个产出指定类型的Source源

- [TaxiRideGenerator.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fsource%2FTaxiRideGenerator.java) 这个封装的不是很好因为更换源的时候不是很好更换

### 在编写job时候如何更好的切换测试和线上呢？

- 保留job中的source sink，然后new RideCleansingExercise时候来指定具体的sink 和 source
- 重写 RideCleansingExercise 中的 execute方法，代替env的
- [RideCleansingExercise.java](src%2Fmain%2Fjava%2Forg%2Fexample%2FexerciseOne%2FRideCleansingExercise.java)
  这个封装的就很好，整个计算图中的source和sink都是构造实例时候传进来的

### 如何在测试过程中构造一个并行发送数据的source？

- 参考 [ParallelTestSource.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FParallelTestSource.java)，注意这里是子任务的概念和数组下标的对应方式

``` 
        TaxiRide toThePole = testRide(-73.9947F, 40.750626F, 0, 90);
        TaxiRide fromThePole = testRide(0, 90, -73.9947F, 40.750626F);
        TaxiRide atPennStation = testRide(-73.9947F, 40.750626F, -73.9947F, 40.750626F);
        TaxiRide atNorthPole = testRide(0, 90, 0, 90);

        // 这里构造了可以发送任何数据一个source，直接构造数据然后加入就可以获取到source
        ParallelTestSource<TaxiRide> source = new ParallelTestSource<>(toThePole, fromThePole, atPennStation, atNorthPole);
```

- ![](https://raw.githubusercontent.com/getyou123/git_pic_use/master/zz202304191645111.png)

### 练习二 有状态的增强(车程及车费)

- 背景主要为了实现一个开始的车程TaxiRide和车费TaxiFare进行匹配，构造产出一个车程车费对象RideAndFare
- 这里假设了 START 和 fare 事件完美配对
- [RidesAndFaresExercise.java](src%2Fmain%2Fjava%2Forg%2Fexample%2FexerciseTwo%2FRidesAndFaresExercise.java)
- [RidesAndFaresExerciseTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FRidesAndFaresExerciseTest.java)

### 如何理解keyedStream

- 调用 keyBy(KeySelector) 实现，Key selector 函数接收单条记录作为输入，返回这条记录的 key。该 key
  可以为任何类型，但是它的计算产生方式必须是具备确定性的
- Key 是“虚拟”的。它们定义为基于实际数据的函数，用以操纵分组算子。 所以实际上不是把数据类型转为key-value形式
- 数据还是整条的流，二期诶
- KeyedStream<WaterSensor, String> 这个类型表示的流中的数据类型是WaterSensor，然后keyed的数据类型是String，即WaterSensor数据流按照String类型的id进行分区

### 关于flink的state

- 算子状态
- Keyed DataStream：作用在指定的key上，
- 广播状态

--- 

### flink中的Keyed State

keyed state 接口提供不同类型状态的访问接口，这些状态都作用于当前输入数据的 key 下。换句话说，这些状态仅可在 KeyedStream
上使用，在Java/Scala API上可以通过 stream.keyBy(...) 得到 KeyedStream，在Python API上可以通过 stream.key_by(...) 得到
KeyedStream。

接下来，我们会介绍不同类型的状态，然后介绍如何使用他们。所有支持的状态类型如下所示：

* ValueState<T>: 保存一个可以更新和检索的值（如上所述，每个值都对应到当前的输入数据的 key，因此算子接收到的每个 key
  都可能对应一个值）。 这个值可以通过 update(T) 进行更新，通过 T value() 进行检索。

* ListState<T>: 保存一个元素的列表。可以往这个列表中追加数据，并在当前的列表上进行检索。可以通过 add(T) 或者 addAll(
  List<T>) 进行添加元素，通过 Iterable<T> get() 获得整个列表。还可以通过 update(List<T>) 覆盖当前的列表。

* ReducingState<T>: 保存一个单值，表示添加到状态的所有值的聚合。接口与 ListState 类似，但使用 add(T) 增加元素，会使用提供的
  ReduceFunction 进行聚合。

* AggregatingState<IN, OUT>: 保留一个单值，表示添加到状态的所有值的聚合。和 ReducingState 相反的是, 聚合类型可能与
  添加到状态的元素的类型不同。 接口与 ListState 类似，但使用 add(IN) 添加的元素会用指定的 AggregateFunction 进行聚合。

* MapState<UK, UV>: 维护了一个映射列表。 你可以添加键值对到状态中，也可以获得反映当前所有映射的迭代器。使用 put(UK，UV) 或者
  putAll(Map<UK，UV>) 添加映射。 使用 get(UK) 检索特定 key。 使用 entries()，keys() 和 values() 分别检索映射、键和值的可迭代视图。你还可以通过
  isEmpty() 来判断是否包含任何键值对。

所有类型的状态还有一个clear() 方法，清除当前 key 下的状态数据，也就是当前输入元素的 key。

请牢记，这些状态对象仅用于与状态交互。状态本身不一定存储在内存中，还可能在磁盘或其他位置。 另外需要牢记的是从状态中获取的值取决于输入元素所代表的
key。 因此，在不同 key 上调用同一个接口，可能得到不同的值。

你必须创建一个 StateDescriptor，才能得到对应的状态句柄。 这保存了状态名称（正如我们稍后将看到的，你可以创建多个状态，并且它们必须具有唯一的名称以便可以引用它们），
状态所持有值的类型，并且可能包含用户指定的函数，例如ReduceFunction。
根据不同的状态类型，可以创建ValueStateDescriptor，ListStateDescriptor， AggregatingStateDescriptor, ReducingStateDescriptor
或 MapStateDescriptor。

状态通过 RuntimeContext 进行访问，因此只能在 rich functions 中使用。请参阅这里获取相关信息， 但是我们很快也会看到一个例子。RichFunction
中 RuntimeContext 提供如下方法：

* ValueState<T> getState(ValueStateDescriptor<T>)
* ReducingState<T> getReducingState(ReducingStateDescriptor<T>)
* ListState<T> getListState(ListStateDescriptor<T>)
* AggregatingState<IN, OUT> getAggregatingState(AggregatingStateDescriptor<IN, ACC, OUT>)
* MapState<UK, UV> getMapState(MapStateDescriptor<UK, UV>)

示例程序：

- [CountWindowAverage.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fhelloworld%2Fstate_learn%2FCountWindowAverage.java)
- [AvgByKeyedStateTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FAvgByKeyedStateTest.java)

### flink中的状态的有效期 TTL

任何类型的 keyed state 都可以有 有效期 (TTL)。如果配置了 TTL 且状态值已过期，则会尽最大可能清除对应的值，这会在后面详述。

所有状态类型都支持单元素的 TTL。 这意味着列表元素和映射元素将独立到期。

在使用状态 TTL 前，需要先构建一个StateTtlConfig 配置对象。 然后把配置传递到 state descriptor 中启用 TTL 功能

```
import org.apache.flink.api.common.state.StateTtlConfig;
import org.apache.flink.api.common.state.ValueStateDescriptor;
import org.apache.flink.api.common.time.Time;

StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.seconds(1)) // 主要是时间是必选的
    .disableCleanupInBackground() // 可以开启 后台线程的清理
    //.cleanupFullSnapshot() 快照是开启清理
    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
    .build();
    
ValueStateDescriptor<String> stateDescriptor = new ValueStateDescriptor<>("text state", String.class);
stateDescriptor.enableTimeToLive(ttlConfig);
```

过期数据的清理：

- 全量快照时进行清理 .cleanupFullSnapshot()
- 增量数据清理 .cleanupIncrementally(10, true)
- 在 RocksDB 压缩时清 .cleanupInRocksdbCompactFilter(1000)

### 算子状态 (Operator State)

算子状态（或者非 keyed 状态）是绑定到一个并行算子实例的状态。Kafka Connector 是 Flink 中使用算子状态一个很具有启发性的例子。Kafka
consumer 每个并行实例维护了 topic partitions 和偏移量的 map 作为它的算子状态。

当并行度改变的时候，算子状态支持将状态重新分发给各并行算子实例。处理重分发过程有多种不同的方案。

在典型的有状态 Flink 应用中你无需使用算子状态。它大都作为一种特殊类型的状态使用。用于实现 source/sink，以及无法对 state
进行分区而没有主键的这类场景中。

### 广播状态 (Broadcast State)

广播状态是一种特殊的算子状态。引入它的目的在于支持一个流中的元素需要广播到所有下游任务的使用情形。在这些任务中广播状态用于保持所有子任务状态相同。
该状态接下来可在第二个处理记录的数据流中访问。可以设想包含了一系列用于处理其他流中元素规则的低吞吐量数据流，这个例子自然而然地运用了广播状态。
考虑到上述这类使用情形，广播状态和其他算子状态的不同之处在于：

它具有 map 格式，
它仅在一些特殊的算子中可用。这些算子的输入为一个广播数据流和非广播数据流，
这类算子可以拥有不同命名的多个广播状态 。


---

## 练习三 窗口分析 (每小时小费)

- 确定每小时赚取最多小费的司机，从 TaxiFare 流中按照窗口进行统计
- 所希望的结果是每小时产生一个 Tuple3<Long, Long, Float> 记录的数据流。 这个记录（Tuple3<Long, Long, Float>
  ）应包含该小时结束时的时间戳（对应三元组的第一个元素）、 该小时内获得小费最多的司机的
  driverId（对应三元组的第二个元素）以及他的实际小费总数（对应三元组的第三个元素））。
- 流转流的结构： 先开窗，然后按照司机id进行聚合，之后这个流在转为从每个司机流中获取最大的

### 误区排查

- 第一步都是按照小时进行开窗，计算窗内（不看前一个窗口）存在tips的driverId，然后数据输出的量级是：窗口内有driverId的计算窗口内的和

``` 
        // 构造的输入数据
        TaxiFare oneFor1In1 = testFare(1, t(0), 1.0F);
        TaxiFare fiveFor1In1 = testFare(1, t(15), 5.0F);
        ----
        TaxiFare tenFor1In2 = testFare(1, t(90), 10.0F);
        TaxiFare twentyFor2In2 = testFare(2, t(90), 20.0F);
        TaxiFare zeroFor3In2 = testFare(3, t(70), 0.0F);
        TaxiFare zeroFor4In2 = testFare(4, t(70), 0.0F);
        TaxiFare oneFor4In2 = testFare(4, t(80), 1.0F);
        TaxiFare tenFor5In2 = testFare(5, t(100), 10.0F);
        
       // 得到的数据输出是，这个是程序外面的时间线无关，因为是按照数据中事件时间算的
        (1577883600000,1,6.0), 
        ----
        (1577887200000,5,10.0), 
        (1577887200000,4,1.0), 
        (1577887200000,3,0.0), 
        (1577887200000,2,20.0), 
        (1577887200000,1,10.0) -- 这里没有driverId = 1 的数据因为上面窗口内没有这个数据
      
```

- 在获取到每个小时内每个司机的sum之后，接下来是需要注意的地方

``` 
  // find the driver with the highest sum of tips for each hour
        DataStream<Tuple3<Long, Long, Float>> hourlyMax =
                hourlyTips.windowAll(TumblingEventTimeWindows.of(Time.hours(1))).maxBy(2);

  -- 因为是开窗的操作，所以还是按照一个窗口内的输出一个 
   [(1577883600000,1,6.0), (1577887200000,2,20.0)]
   
   如果按照
   DataStream<Tuple3<Long, Long, Float>> hourlyMax = hourlyTips
    .keyBy(t -> t.f0)
    .maxBy(2);
    
    这个逻辑来写的话，那就会导致数据的输出是这样的
    (1577883600000,1,6.0), 
    (1577887200000,5,10.0), 
    (1577887200000,5,10.0), 
    (1577887200000,5,10.0), 
    (1577887200000,2,20.0), 
    (1577887200000,2,20.0)]
    可以看到是上游hourlyTips的每条记录都产出了一条记录，而不是两条记录
```

### 如何设置flink中的水印和时间戳呢？

- 一般来说，实际时间戳来自数据中的某个字段，使用TimestampAssigner去设置
- 同时时间戳的分配与 watermark 的生成是齐头并进的，水印用来告知flink程序的时间进度，使用 WatermarkGenerator 来设置
- `WatermarkStrategy` 工具类整合了 `TimestampAssigner` 和 `WatermarkGenerator` ，在source时候直接设置获取水印和时间戳

``` 
WatermarkStrategy
        .<Tuple2<Long, String>>forBoundedOutOfOrderness(Duration.ofSeconds(20)) // 最大混乱程度的
        .withTimestampAssigner((event, timestamp) -> event.f0);
```

### flink中的source如何理解呢？

- 作为数据输入流中的起点
- 其本质就是一个SourceFunction<?>，然后在env中addSource即可

```
SourceFunction
    |
    +---ParallelSourceFunction 支持并行度的
    |
    +---RichSourceFunction     支持一些前置操作的
```

- [Source01Test.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FSource01Test.java) 展示了多个数据源的方式
- [MySource.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fhelloworld%2FSourceLearn%2FMySource.java) 展示了自定义source的方式

### flink中的source如何设置并行度呢？

- 如何判定是按照几个并行度去读取数据的呢
- flink中的一个ParallelSourceFunction 可以支持设置并行度
  `DataStream<String> stream = env.addSource(kafkaConsumer).setParallelism(4);`
- [ParallelTestSource.java](src%2Ftest%2Fjava%2Forg%2Fexample%2Ftool%2FParallelTestSource.java) 这个就是一个很好的案例

### 如何理解flink中的map function

- map 算子其实就是需要一个MapFunction，然后按照在多个并行度上启动执行的mapFunction
- 这个map function是一个并行度上执行一个函数，这个是和并行度是绑定的
- 同理的flatmap需要的是一个flatmapFunction，filter需要的是一个filterFunction
- [FlatMapFunctionTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FFlatMapFunctionTest.java)
- [MapFunctionTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FMapFunctionTest.java)

### flink中如何理解并行度和slot

- 每个计算过程为了对付大数据场景，需要把计算逻辑分为多个子任务，比如 map操作，是对于100条数据的处理，分成50条数据一组作为作为一个子task，那么这个任务的并行度就是2
- 实际有多少资源可用是对应的slot，上面的例子中会要求有2个工人去干活，那么实际可以干活的工人不能少于2个
- .setParallelism(4) 就是在任务层面进行划分子任务，当然分配到slot之后可能是在同一个taskmanager上

### 如何理解flink中的connect两个流？connect和 union的区别是啥？

- connect两个流是貌合神离的 DataStream[A], DataStream[B] -> ConnectedStreams[A,B]
- connect两个流中的数据不要求是一样的，数据类型可以是不同的
- [RidesAndFaresExercise.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fexercise02%2FRidesAndFaresExercise.java) 是一个很好的例子
- union中的两个流的数据类型要求是一致的，可以操作多个流进行union

### flink中的简单的滚动算子？

基于KeyedStream流，里面的内容最好是POJO，scala中的case 类，或者元组类型

- sum, 分组求和
- min, 分区求最小值
- max 分组求最大值
- minBy, 分组求组内的最小值的那条记录
- maxBy 分组求最内的最大值的那条记录
- reduce 分组进行组内的reduce方法
- 操作之后的数据类型发生了变化 KeyedStream -> SingleOutputStreamOperator

### POJO类的定义？

该类型必须是公共类，即必须使用 public 修饰符修饰。
该类型必须具有公共的无参构造函数。
该类型的属性必须是公共属性，即必须使用 public 修饰符修饰，并且必须具有对应的 get/set 方法。

### flink中的min max sum等底层原理？maxBy 和 max啥区别呢？

- [SimpleAgrTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FSimpleAgrTest.java)
- min max这类算子类似mr，分区之后，在局部聚合和全局聚合过程中，Flink 会使用状态管理器来维护中间状态，以支持有状态的计算
- minBy maxBy这些是返回最大值对应的那条记录，而min max等返回的是最大值，这是不同的
- minBy maxBy 支持第二个参数 .maxBy("temperature",true); 同样的最大值保留不保留第一条的作为结果

### flink中的process方法

- 这个是个比较底层的算子，ProcessFunction 可以访问比较底层的 事件时间，水位线，注册定时器
- 针对非keyed 的流： ProcessFunction 中最重要的方法是 processElement，它会在每个输入元素到达时调用，可以对元素进行处理，并输出新的元素或通过
  Output 接口向侧输出流输出元素。
- 针对keyed的流 ；KeyProcessFunction 来处理，可以获取key等操作
- @TODO 获取key 获取处理时间 获取事件时间 设置处理时间的定时器 设置事件时间的定时器

``` 
Context 对象提供了许多与时间和定时器相关的方法： 

timestamp() 方法可以获取当前元素的时间戳（即事件时间或者处理时间）。

getCurrentKey() 方法可以获取当前元素的键（即分组键）。

timerService().currentProcessingTime() 方法可以获取当前的处理时间。

timerService().currentWatermark() 方法可以获取当前的水位线。

timerService().registerProcessingTimeTimer() 方法可以注册一个处理时间定时器，当处理时间到达指定时间时，会触发 onTimer() 方法。

timerService().registerEventTimeTimer() 方法可以注册一个事件时间定时器，当事件时间到达指定时间时，会触发 onTimer() 方法。

timerService().deleteProcessingTimeTimer() 方法可以删除一个处理时间定时器。

timerService().deleteEventTimeTimer() 方法可以删除一个事件时间定时器。

```

### flink中如何对流进行分区？

- keyBy 进行分区 先按照key分组, 按照key的双重hash来选择后面的分区
- rebalance() 方法会将数据流重新平衡，将数据均匀地分配到所有的分区中，适用于数据倾斜的场景。
- rescale() 方法会将数据流按照哈希值进行分区，但是分区的数量不变，适用于数据均匀分布的场景。
- shuffle() 方法会将数据流随机打乱，然后按照哈希值进行分区，适用于需要随机化数据分布的场景。
- forward() 方法会将数据流发送到下游算子的并行子任务中，适用于数据流只需要在同一个算子中进行处理的场景。

### flink中的sink如何理解？

- 作为数据的输出位置
- sink其本质也就是sinkFunction stream.addSink(sinkFunction);
- 一些内置的sink：![](https://raw.githubusercontent.com/getyou123/git_pic_use/master/zz202304271140162.png)
- 自定义一个sink [TestSink.java](src%2Ftest%2Fjava%2Forg%2Fexample%2Ftool%2FTestSink.java)
- kafkaSink [KafkaSinkTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FKafkaSinkTest.java)
- redisSink [RedisSinkTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FRedisSinkTest.java)
- ElasticsearchSink [ElasticsearchSinkTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FElasticsearchSinkTest.java)
- 自己封装的 使用jdbc的写入到MySQL的[MySinkFunction.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fhelloworld%2FSinkFunctionLearn%2FMySinkFunction.java)
- 自己封装的使用 druid连接池管理jdbc连接的版本[MySinkFunctionWithPool.java](src%2Fmain%2Fjava%2Forg%2Fexample%2Fhelloworld%2FSinkFunctionLearn%2FMySinkFunctionWithPool.java)


### flink中的执行模式？BATCH 和 STREAMING 的区别？
- 在流式api上增加了使用BATCH的模式
- 区分于 STREAMING  模式
- 一个公用的规则就是: 当你处理的数据是有界的就应该使用BATCH执行模式, 因为它更加高效. 当你的数据是无界的, 则必须使用STREAMING 执行模式, 因为只有这种模式才能处理持续的数据流.
- 通过命令配置 `bin/flink run -Dexecution.runtime-mode=BATCH ...`
- 有界数据用STREAMING和BATCH的区别
``` 
STREAMING模式下, 数据是来一条输出一次结果.
BATCH模式下, 数据处理完之后, 一次性输出结果.
```

### 如何理解flink中的window？理解要点 WindowAssigner 最大乱序程度，水印，自己写个WatermarkStrategy WatermarkGenerator
- 无界流中截取一段放在某个bucket中，然后基于这部分的静态数据进行计算
- ![](https://raw.githubusercontent.com/getyou123/git_pic_use/master/zz202304271813606.png)
- 窗口分为两类窗口：
  - 基于时间的
  - 基于count的
- 如何确定某条数据放在哪几个窗口呢？ -- WindowAssigner 来定
- 场景：5秒一个滚动窗口，最大乱序程度3s 情况下进行计算 
  - ![](https://raw.githubusercontent.com/getyou123/git_pic_use/master/zz202304281620863.png)
  - [WaterMarkLearnTest.java](src%2Ftest%2Fjava%2Forg%2Fexample%2FWaterMarkLearnTest.java)
  - 自定义实现的WaterMark： [WaterMarkLearn](src%2Fmain%2Fjava%2Forg%2Fexample%2Fhelloworld%2FWaterMarkLearn)